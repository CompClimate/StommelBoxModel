# @package _global_

# example hyperparameter optimization of some experiment with Optuna:
# python train.py -m hparams_search=lr_batch_size experiment=example

defaults:
  - override /hydra/sweeper: optuna

optimized_metric: "val/loss_best"

hydra:
  mode: "MULTIRUN"

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper

    storage: null

    study_name: null

    n_jobs: 1

    direction: minimize

    n_trials: 20

    sampler:
      _target_: optuna.samplers.GridSampler
      seed: 1234
      n_startup_trials: 10

    params:
      ++model.optimizer.lr: range(0.0001, 0.1, step=0.001)
      ++data.batch_size: choice(32, 64, 128, 256)
